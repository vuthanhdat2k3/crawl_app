import os
import json
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

BASE_URL = "https://nettruyen.me.uk/trang-chu"

# Ki·ªÉm tra xem ƒëang ch·∫°y tr√™n Vercel (serverless) hay local
IS_VERCEL = os.environ.get('VERCEL', False) or os.environ.get('VERCEL_ENV', False)

# Tr√™n Vercel, ch·ªâ /tmp m·ªõi c√≥ th·ªÉ ghi ƒë∆∞·ª£c
if IS_VERCEL:
    DATA_DIR = "/tmp/data"
    USER_DATA_DIR = "/tmp/crawler/browser_profile"
else:
    DATA_DIR = "data"
    USER_DATA_DIR = "crawler/browser_profile"

# Ch·ªâ t·∫°o th∆∞ m·ª•c khi kh√¥ng ch·∫°y trong serverless context ho·∫∑c khi s·ª≠ d·ª•ng /tmp
try:
    os.makedirs(DATA_DIR, exist_ok=True)
except OSError:
    pass  # B·ªè qua l·ªói n·∫øu kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c

def crawl_home():
    with sync_playwright() as p:
        # S·ª≠ d·ª•ng persistent context ƒë·ªÉ gi·ªØ session v√† n√© bot
        context = p.chromium.launch_persistent_context(
            USER_DATA_DIR,
            headless=False,
            args=["--disable-blink-features=AutomationControlled"]
        )
        page = context.new_page()
        print("üåç ƒêang truy c·∫≠p trang ch·ªß NetTruyen...")
        
        page.goto(BASE_URL, wait_until="networkidle", timeout=60000)
        
        # L·∫•y n·ªôi dung HTML sau khi ƒë√£ load xong
        content = page.content()
        soup = BeautifulSoup(content, "lxml")
        
        manga_list = []
        items = soup.select(".item")
        
        for item in items:
            title_el = item.select_one("h3 a")
            img_el = item.select_one("img")
            
            if title_el:
                manga_list.append({
                    "id": title_el['href'].split('/')[-1],
                    "title": title_el.get_text(strip=True),
                    "url": title_el['href'],
                    "thumbnail": img_el['data-original'] if img_el.has_attr('data-original') else img_el['src']
                })

        # L∆∞u v√†o file JSON ƒë·ªÉ Web Flask s·ª≠ d·ª•ng
        with open(os.path.join(DATA_DIR, "manga_list.json"), "w", encoding="utf-8") as f:
            json.dump(manga_list, f, ensure_ascii=False, indent=4)
            
        print(f"‚úÖ ƒê√£ l∆∞u {len(manga_list)} truy·ªán v√†o data/manga_list.json")
        context.close()

if __name__ == "__main__":
    crawl_home()