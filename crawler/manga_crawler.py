"""
Manga Crawler - Thu th·∫≠p d·ªØ li·ªáu t·ª´ NetTruyen
L∆∞u tr·ªØ: CH·ªà S·ª¨ D·ª§NG CLOUD (MongoDB + ImageKit.io)
Kh√¥ng s·ª≠ d·ª•ng local storage
"""

import os
import re
import sys
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# Import database v√† image storage
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from database import db
from imagekit_storage import image_storage

class MangaCrawler:
    def __init__(self):
        self.base_url = "https://nettruyen.me.uk/"
        self.user_data_dir = os.path.join(os.path.dirname(__file__), "browser_profile")
        
        # K·∫øt n·ªëi cloud storage
        db.connect()
        image_storage.connect()
        print("‚òÅÔ∏è Cloud-Only Mode: MongoDB + ImageKit")

    def _get_browser_context(self, playwright):
        """T·∫°o browser context v·ªõi anti-bot"""
        return playwright.chromium.launch_persistent_context(
            self.user_data_dir,
            headless=True,
            args=["--disable-blink-features=AutomationControlled"],
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36"
        )

    def upload_cover(self, page, manga_id, thumbnail_url):
        """T·∫£i v√† upload ·∫£nh b√¨a l√™n ImageKit"""
        if not thumbnail_url:
            return None
        
        try:
            # T·∫£i ·∫£nh t·ª´ ngu·ªìn
            response = page.request.get(thumbnail_url, headers={"referer": self.base_url + "/"})
            if response.status == 200:
                # Upload tr·ª±c ti·∫øp l√™n ImageKit
                image_bytes = response.body()
                url = image_storage.upload_from_bytes(
                    image_bytes, 
                    "manga/covers", 
                    f"{manga_id}.jpg"
                )
                if url:
                    print(f"  ‚òÅÔ∏è Uploaded cover: {manga_id}")
                    return url
        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói upload cover {manga_id}: {e}")
        
        return thumbnail_url  # Fallback v·ªÅ URL g·ªëc

    def crawl_home(self, download_covers=True):
        """Crawl danh s√°ch manga t·ª´ trang ch·ªß - L∆ØU V√ÄO MONGODB"""
        print("üåç ƒêang crawl trang ch·ªß NetTruyen...")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(self.base_url, wait_until="networkidle", timeout=60000)
            
            # Cu·ªôn ƒë·ªÉ load th√™m
            for _ in range(3):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(1000)
            
            content = page.content()
            soup = BeautifulSoup(content, "lxml")
            
            manga_list = []
            items = soup.select(".item")
            
            for idx, item in enumerate(items):
                title_el = item.select_one("h3 a")
                img_el = item.select_one("img")
                
                if title_el:
                    href = title_el.get('href', '')
                    manga_id = href.split('/')[-1] if href else ''
                    
                    # L·∫•y thumbnail g·ªëc
                    thumbnail_original = ""
                    if img_el:
                        thumbnail_original = img_el.get('data-original') or img_el.get('data-src') or img_el.get('src', '')
                    
                    # Upload cover l√™n ImageKit
                    thumbnail = thumbnail_original
                    if download_covers and thumbnail_original:
                        uploaded_url = self.upload_cover(page, manga_id, thumbnail_original)
                        if uploaded_url:
                            thumbnail = uploaded_url
                    
                    # L·∫•y chapter m·ªõi nh·∫•t
                    latest_chapter = ""
                    chapter_el = item.select_one(".comic-item .chapter a") or item.select_one(".chapter a")
                    if chapter_el:
                        latest_chapter = chapter_el.get_text(strip=True)
                    
                    manga_list.append({
                        "id": manga_id,
                        "title": title_el.get_text(strip=True),
                        "url": href,
                        "thumbnail": thumbnail,
                        "thumbnail_original": thumbnail_original,
                        "latest_chapter": latest_chapter
                    })
                    
                    print(f"  [{idx+1}/{len(items)}] {title_el.get_text(strip=True)[:30]}...")
            
            # L∆∞u v√†o MongoDB
            db.save_manga_list(manga_list)
            print(f"‚òÅÔ∏è ƒê√£ l∆∞u {len(manga_list)} truy·ªán v√†o MongoDB")
            
            context.close()
            return manga_list

    def crawl_story_detail(self, manga_id, download_cover=True):
        """Crawl chi ti·∫øt m·ªôt truy·ªán - L∆ØU V√ÄO MONGODB"""
        url = f"{self.base_url}/truyen-tranh/{manga_id}"
        print(f"üìñ ƒêang crawl chi ti·∫øt truy·ªán: {manga_id}")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            page.wait_for_timeout(2000)
            
            print("üìú ƒêang ph√¢n t√≠ch chapters...")
            
            content = page.content()
            soup = BeautifulSoup(content, "lxml")
            
            # L·∫•y th√¥ng tin truy·ªán
            title = ""
            title_el = soup.select_one("h1.title-detail")
            if title_el:
                title = title_el.get_text(strip=True)
            
            description = ""
            desc_el = soup.select_one(".detail-content p")
            if desc_el:
                description = desc_el.get_text(strip=True)
            
            # L·∫•y v√† upload thumbnail
            thumbnail_original = ""
            thumbnail = ""
            thumb_el = soup.select_one(".col-image img")
            if thumb_el:
                thumbnail_original = thumb_el.get('data-original') or thumb_el.get('data-src') or thumb_el.get('src', '')
            
            if download_cover and thumbnail_original:
                thumbnail = self.upload_cover(page, manga_id, thumbnail_original)
            else:
                thumbnail = thumbnail_original
            
            # L·∫•y th·ªÉ lo·∫°i
            genres = []
            genre_els = soup.select(".kind.row .col-xs-8 a")
            for g in genre_els:
                genres.append(g.get_text(strip=True))
            
            # L·∫•y t√°c gi·∫£
            author = ""
            author_el = soup.select_one(".author.row .col-xs-8")
            if author_el:
                author = author_el.get_text(strip=True)
            
            # Ph√¢n t√≠ch pattern chapters
            visible_rows = soup.select("#nt_listchapter ul li.row:not(.heading)")
            
            chapters = []
            chapter_pattern = None
            max_chapter = 0
            min_chapter = float('inf')
            
            for row in visible_rows:
                link = row.select_one("a")
                if link:
                    chap_url = link.get('href', '')
                    
                    url_match = re.search(r'[/-](chuong|chap|chapter)[/-]?(\d+)', chap_url, re.IGNORECASE)
                    if url_match:
                        chapter_num = int(url_match.group(2))
                        prefix = url_match.group(1).lower()
                        
                        if not chapter_pattern:
                            base_url = re.sub(r'[/-](chuong|chap|chapter)[/-]?\d+.*$', '', chap_url, flags=re.IGNORECASE)
                            chapter_pattern = {
                                'base_url': base_url,
                                'prefix': prefix,
                                'separator': '-' if f'{prefix}-' in chap_url.lower() else ''
                            }
                        
                        max_chapter = max(max_chapter, chapter_num)
                        min_chapter = min(min_chapter, chapter_num)
            
            print(f"  üìä Ph√¢n t√≠ch: Chapter {min_chapter} ‚Üí {max_chapter}")
            
            # Generate t·∫•t c·∫£ chapters
            if chapter_pattern and max_chapter > 0:
                for i in range(max_chapter, -1, -1):
                    chap_id = f"{chapter_pattern['prefix']}{chapter_pattern['separator']}{i}"
                    chap_url = f"{chapter_pattern['base_url']}/{chap_id}"
                    
                    if not chap_url.startswith('http'):
                        chap_url = self.base_url + chap_url
                    
                    chapters.append({
                        "id": chap_id,
                        "name": f"Chapter {i}",
                        "url": chap_url
                    })
                
                print(f"  ‚úÖ ƒê√£ generate {len(chapters)} chapters!")
            else:
                # Fallback
                for row in visible_rows:
                    link = row.select_one("a")
                    if link:
                        chap_url = link.get('href', '')
                        chap_id = chap_url.split('/')[-1] if chap_url else ''
                        if not chap_url.startswith('http'):
                            chap_url = self.base_url + chap_url
                        chapters.append({
                            "id": chap_id,
                            "name": link.get_text(strip=True),
                            "url": chap_url
                        })
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu
            data = {
                "id": manga_id,
                "title": title,
                "description": description,
                "thumbnail": thumbnail,
                "thumbnail_original": thumbnail_original,
                "author": author,
                "genres": genres,
                "chapters": chapters,
                "total_chapters": len(chapters)
            }
            
            # L∆∞u v√†o MongoDB
            db.save_manga_detail(data)
            print(f"‚òÅÔ∏è ƒê√£ l∆∞u '{title}' v·ªõi {len(chapters)} chapters v√†o MongoDB")
            
            context.close()
            return data

    def download_chapter_images(self, manga_id, chapter_id, chapter_url=None):
        """T·∫£i v√† upload ·∫£nh chapter l√™n ImageKit - L∆ØU URLs V√ÄO MONGODB"""
        if not chapter_url:
            chapter_url = f"{self.base_url}/truyen-tranh/{manga_id}/{chapter_id}"
        
        # Ki·ªÉm tra ƒë√£ c√≥ tr√™n cloud ch∆∞a
        existing_urls = db.get_chapter_images(manga_id, chapter_id)
        if existing_urls:
            print(f"‚è≠Ô∏è Chapter {chapter_id} ƒë√£ c√≥ tr√™n cloud ({len(existing_urls)} ·∫£nh)")
            return existing_urls
        
        print(f"üì• ƒêang t·∫£i v√† upload chapter: {chapter_id}")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            # Enable Stealth Mode
            from playwright_stealth import stealth_sync
            stealth_sync(page)
            
            page.goto(chapter_url, wait_until="domcontentloaded", timeout=60000)
            
            # BYPASS LOGIC 2.0
            max_retries = 3
            for attempt in range(max_retries):
                page_title = page.title()
                print(f"  üìÑ [{attempt+1}/{max_retries}] Page Title: {page_title}")
                
                if "Just a moment" in page_title or "Attention Required" in page_title or "Cloudflare" in page_title:
                    print("  üõ°Ô∏è Detect Cloudflare! Waiting for redirect...")
                    page.wait_for_timeout(5000)
                    
                    # Th·ª≠ click v√†o b·∫•t k·ª≥ iframe/checkbox n√†o n·∫øu c√≥ (basic attempt)
                    try:
                         frames = page.frames
                         for frame in frames:
                             if "challenge" in frame.url:
                                 print("  üñ±Ô∏è Found Challenge Frame, trying to interact...")
                                 frame.click("body", timeout=2000)
                    except: pass
                    
                    page.wait_for_timeout(5000)
                else:
                    # ƒê√£ v√†o ƒë∆∞·ª£c trang ch√≠nh
                    break
            
            # Cu·ªôn trang ch·∫≠m h∆°n ƒë·ªÉ gi·∫£ l·∫≠p ng∆∞·ªùi d√πng
            print("üìú ƒêang k√≠ch ho·∫°t lazy loading...")
            for i in range(10): # TƒÉng s·ªë l·∫ßn cu·ªôn
                page.mouse.wheel(0, 1000) # D√πng mouse wheel thay v√¨ scrollTo cho gi·ªëng ng∆∞·ªùi
                page.wait_for_timeout(1000)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(3000)
            
            # Ch·ªù ·∫£nh xu·∫•t hi·ªán (quan tr·ªçng)
            try:
                page.wait_for_selector(".reading-detail img, .page-chapter img", timeout=5000)
            except:
                print("  ‚ö†Ô∏è Timeout ch·ªù ·∫£nh, th·ª≠ selector kh√°c...")

            # L·∫•y t·∫•t c·∫£ ·∫£nh with enhanced selectors
            imgs = page.query_selector_all(".reading-detail img, .page-chapter img, .reading img, #image-0")
            
            urls = []
            folder_path = f"manga/{manga_id}/{chapter_id}"
            
            print(f"‚òÅÔ∏è T√¨m th·∫•y {len(imgs)} element ·∫£nh. B·∫Øt ƒë·∫ßu upload...")
            
            for idx, img in enumerate(imgs):
                # Th·ª≠ nhi·ªÅu attribute ch·ª©a link ·∫£nh
                src = img.get_attribute("data-original") or img.get_attribute("data-src") or img.get_attribute("src")
                
                if not src:
                    continue
                    
                if "http" not in src:
                     if src.startswith("//"):
                         src = "https:" + src
                     else:
                         continue

                try:
                    response = page.request.get(src, headers={"referer": self.base_url + "/"})
                    if response.status == 200:
                        filename = f"{idx:03d}.jpg"
                        
                        # Upload tr·ª±c ti·∫øp l√™n ImageKit
                        url = image_storage.upload_from_bytes(
                            response.body(),
                            folder_path,
                            filename
                        )
                        
                        if url:
                            urls.append(url)
                            print(f"  ‚òÅÔ∏è [{idx+1}/{len(imgs)}] Uploaded")
                        else:
                            print(f"  ‚ùå [{idx+1}/{len(imgs)}] Upload failed")
                except Exception as e:
                    print(f"  ‚ùå L·ªói ·∫£nh {idx}: {e}")
            
            context.close()
            
            # L∆∞u URLs v√†o MongoDB
            if urls:
                db.save_chapter_images(manga_id, chapter_id, urls)
                print(f"‚òÅÔ∏è ƒê√£ l∆∞u {len(urls)} URLs v√†o MongoDB")
            
            return urls

    def get_manga_list(self):
        """L·∫•y danh s√°ch manga t·ª´ MongoDB"""
        mangas = db.get_manga_list(limit=200)
        # Chuy·ªÉn ObjectId th√†nh string
        for m in mangas:
            if '_id' in m:
                m['_id'] = str(m['_id'])
        return mangas

    def get_story_data(self, manga_id):
        """L·∫•y chi ti·∫øt truy·ªán t·ª´ MongoDB"""
        data = db.get_manga_detail(manga_id)
        if data and '_id' in data:
            data['_id'] = str(data['_id'])
        return data

    def get_chapter_images(self, manga_id, chapter_id):
        """L·∫•y danh s√°ch URLs ·∫£nh t·ª´ MongoDB"""
        return db.get_chapter_images(manga_id, chapter_id)
    
    def get_download_status(self, manga_id):
        """L·∫•y tr·∫°ng th√°i t·∫£i t·ª´ MongoDB"""
        return db.get_download_status(manga_id)
        
    def get_downloaded_chapters(self, manga_id):
        """L·∫•y danh s√°ch chapter IDs ƒë√£ t·∫£i"""
        return db.get_downloaded_chapters(manga_id)


# CLI Interface
if __name__ == "__main__":
    import sys
    
    crawler = MangaCrawler()
    
    if len(sys.argv) < 2:
        print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           üéå MANGA CRAWLER - CLOUD ONLY MODE üéå           ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  L∆∞u tr·ªØ: MongoDB (data) + ImageKit (·∫£nh 20GB free)       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  C√°ch s·ª≠ d·ª•ng:                                            ‚ïë
‚ïë   python manga_crawler.py home                            ‚ïë
‚ïë   python manga_crawler.py story <manga-id>                ‚ïë
‚ïë   python manga_crawler.py chapter <manga-id> <chapter-id> ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """)
        sys.exit(0)
    
    command = sys.argv[1]
    
    if command == "home":
        crawler.crawl_home()
    
    elif command == "story" and len(sys.argv) >= 3:
        manga_id = sys.argv[2]
        crawler.crawl_story_detail(manga_id)
    
    elif command == "chapter" and len(sys.argv) >= 4:
        manga_id = sys.argv[2]
        chapter_id = sys.argv[3]
        crawler.download_chapter_images(manga_id, chapter_id)
    
    else:
        print("‚ùå L·ªánh kh√¥ng h·ª£p l·ªá!")
        print("S·ª≠ d·ª•ng: python manga_crawler.py [home|story|chapter] [args...]")
