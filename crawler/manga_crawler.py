"""
Manga Crawler - Thu th·∫≠p d·ªØ li·ªáu t·ª´ NetTruyen
L∆∞u tr·ªØ: CH·ªà S·ª¨ D·ª§NG CLOUD (MongoDB + ImageKit.io)
Kh√¥ng s·ª≠ d·ª•ng local storage
∆Øu ti√™n FlareSolverr ƒë·ªÉ bypass Cloudflare (c·∫£ local v√† production)
"""

import os
import re
import sys
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup

# Import database v√† image storage
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from database import db
from imagekit_storage import image_storage
from crawler.flaresolverr_client import flaresolverr

class MangaCrawler:
    def __init__(self):
        self.base_url = "https://nettruyen.me.uk"
        self.user_data_dir = os.path.join(os.path.dirname(__file__), "browser_profile")
        
        # K·∫øt n·ªëi cloud storage
        db.connect()
        image_storage.connect()
        print("‚òÅÔ∏è Cloud-Only Mode: MongoDB + ImageKit")
        
        # Ki·ªÉm tra FlareSolverr (∆∞u ti√™n d√πng c·∫£ local v√† production)
        self.use_flaresolverr = flaresolverr.check_connection()
        if self.use_flaresolverr:
            print("üöÄ FlareSolverr Mode: ∆Øu ti√™n FlareSolverr cho t·∫•t c·∫£ requests")
        
        # Session cho requests (d√πng cookies t·ª´ FlareSolverr)
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
            "Referer": self.base_url
        })
        
        # Cookies t·ª´ FlareSolverr ƒë·ªÉ bypass Cloudflare
        self.cf_cookies = None

    def _get_browser_context(self, playwright):
        """T·∫°o browser context v·ªõi anti-bot (fallback khi kh√¥ng c√≥ FlareSolverr)"""
        return playwright.chromium.launch_persistent_context(
            self.user_data_dir,
            headless=True,
            args=["--disable-blink-features=AutomationControlled"],
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36"
        )

    def _update_session_cookies(self, cookies):
        """C·∫≠p nh·∫≠t cookies t·ª´ FlareSolverr v√†o session requests"""
        self.cf_cookies = cookies
        for cookie in cookies:
            self.session.cookies.set(cookie.get("name"), cookie.get("value"))

    def upload_cover_via_requests(self, manga_id, thumbnail_url):
        """T·∫£i v√† upload ·∫£nh b√¨a l√™n ImageKit s·ª≠ d·ª•ng requests (cho FlareSolverr)"""
        if not thumbnail_url:
            return None
        
        try:
            # D√πng cookies t·ª´ FlareSolverr n·∫øu c√≥
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
                "Referer": self.base_url,
                "Accept": "image/webp,image/apng,image/*,*/*;q=0.8"
            }
            
            # Set cookies t·ª´ FlareSolverr
            cookies = {}
            if self.cf_cookies:
                for cookie in self.cf_cookies:
                    cookies[cookie.get('name')] = cookie.get('value')
            
            response = requests.get(thumbnail_url, headers=headers, cookies=cookies, timeout=30)
            if response.status_code == 200 and len(response.content) > 1000:
                # Upload l√™n ImageKit
                url = image_storage.upload_from_bytes(
                    response.content, 
                    "manga/covers", 
                    f"{manga_id}.jpg"
                )
                if url:
                    print(f"  ‚òÅÔ∏è Uploaded cover: {manga_id}")
                    return url
        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói upload cover {manga_id}: {e}")
        
        return thumbnail_url  # Fallback v·ªÅ URL g·ªëc

    def upload_cover(self, page, manga_id, thumbnail_url):
        """T·∫£i v√† upload ·∫£nh b√¨a l√™n ImageKit"""
        if not thumbnail_url:
            return None
        
        try:
            # T·∫£i ·∫£nh t·ª´ ngu·ªìn
            response = page.request.get(thumbnail_url, headers={"referer": self.base_url + "/"})
            if response.status == 200:
                # Upload tr·ª±c ti·∫øp l√™n ImageKit
                image_bytes = response.body()
                url = image_storage.upload_from_bytes(
                    image_bytes, 
                    "manga/covers", 
                    f"{manga_id}.jpg"
                )
                if url:
                    print(f"  ‚òÅÔ∏è Uploaded cover: {manga_id}")
                    return url
        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói upload cover {manga_id}: {e}")
        
        return thumbnail_url  # Fallback v·ªÅ URL g·ªëc

    def crawl_home(self, download_covers=True):
        """Crawl danh s√°ch manga t·ª´ trang ch·ªß - L∆ØU V√ÄO MONGODB"""
        print("üåç ƒêang crawl trang ch·ªß NetTruyen...")
        
        # ∆Øu ti√™n FlareSolverr
        if self.use_flaresolverr:
            manga_list = self._crawl_home_via_flaresolverr(download_covers)
            if manga_list:
                return manga_list
            print("‚ö†Ô∏è FlareSolverr th·∫•t b·∫°i, th·ª≠ Playwright...")
        
        # Fallback: Playwright
        return self._crawl_home_via_playwright(download_covers)
    
    def _crawl_home_via_flaresolverr(self, download_covers=True):
        """Crawl trang ch·ªß qua FlareSolverr"""
        print("üîì ƒêang crawl trang ch·ªß qua FlareSolverr...")
        
        result = flaresolverr.get_page(self.base_url)
        if not result or not result.get("html"):
            print("‚ùå FlareSolverr kh√¥ng th·ªÉ l·∫•y ƒë∆∞·ª£c trang")
            return None
        
        # L∆∞u cookies ƒë·ªÉ d√πng cho c√°c request kh√°c
        self._update_session_cookies(result.get("cookies", []))
        
        html = result["html"]
        soup = BeautifulSoup(html, "lxml")
        
        manga_list = []
        items = soup.select(".item")
        
        # Thu th·∫≠p th√¥ng tin tr∆∞·ªõc
        manga_data = []
        for item in items:
            title_el = item.select_one("h3 a")
            img_el = item.select_one("img")
            
            if title_el:
                href = title_el.get('href', '')
                manga_id = href.split('/')[-1] if href else ''
                
                thumbnail_original = ""
                if img_el:
                    thumbnail_original = img_el.get('data-original') or img_el.get('data-src') or img_el.get('src', '')
                
                latest_chapter = ""
                chapter_el = item.select_one(".comic-item .chapter a") or item.select_one(".chapter a")
                if chapter_el:
                    latest_chapter = chapter_el.get_text(strip=True)
                
                manga_data.append({
                    "id": manga_id,
                    "title": title_el.get_text(strip=True),
                    "url": href,
                    "thumbnail_original": thumbnail_original,
                    "latest_chapter": latest_chapter
                })
        
        # Upload covers song song n·∫øu c·∫ßn
        if download_covers and manga_data:
            print(f"‚òÅÔ∏è Upload {len(manga_data)} covers song song...")
            thumbnails = self._upload_covers_parallel(manga_data)
            for i, manga in enumerate(manga_data):
                manga["thumbnail"] = thumbnails.get(manga["id"], manga["thumbnail_original"])
        
        # T·∫°o manga_list
        for manga in manga_data:
            manga_list.append({
                "id": manga["id"],
                "title": manga["title"],
                "url": manga["url"],
                "thumbnail": manga.get("thumbnail", manga["thumbnail_original"]),
                "thumbnail_original": manga["thumbnail_original"],
                "latest_chapter": manga["latest_chapter"]
            })
            print(f"  ‚úÖ {manga['title'][:40]}...")
        
        # L∆∞u v√†o MongoDB
        db.save_manga_list(manga_list)
        print(f"‚òÅÔ∏è ƒê√£ l∆∞u {len(manga_list)} truy·ªán v√†o MongoDB")
        
        return manga_list
    
    def _upload_covers_parallel(self, manga_data):
        """Upload nhi·ªÅu cover song song"""
        results = {}
        
        def upload_one(manga):
            thumbnail = self.upload_cover_via_requests(manga["id"], manga["thumbnail_original"])
            return manga["id"], thumbnail
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(upload_one, m): m["id"] for m in manga_data if m["thumbnail_original"]}
            for future in as_completed(futures):
                try:
                    manga_id, thumbnail = future.result()
                    results[manga_id] = thumbnail
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Upload cover l·ªói: {e}")
        
        return results
    
    def _crawl_home_via_playwright(self, download_covers=True):
        """Crawl trang ch·ªß qua Playwright (fallback)"""
        from playwright.sync_api import sync_playwright
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(self.base_url, wait_until="networkidle", timeout=60000)
            
            # Cu·ªôn ƒë·ªÉ load th√™m
            for _ in range(3):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(1000)
            
            content = page.content()
            soup = BeautifulSoup(content, "lxml")
            
            manga_list = []
            items = soup.select(".item")
            
            for idx, item in enumerate(items):
                title_el = item.select_one("h3 a")
                img_el = item.select_one("img")
                
                if title_el:
                    href = title_el.get('href', '')
                    manga_id = href.split('/')[-1] if href else ''
                    
                    # L·∫•y thumbnail g·ªëc
                    thumbnail_original = ""
                    if img_el:
                        thumbnail_original = img_el.get('data-original') or img_el.get('data-src') or img_el.get('src', '')
                    
                    # Upload cover l√™n ImageKit
                    thumbnail = thumbnail_original
                    if download_covers and thumbnail_original:
                        uploaded_url = self.upload_cover(page, manga_id, thumbnail_original)
                        if uploaded_url:
                            thumbnail = uploaded_url
                    
                    # L·∫•y chapter m·ªõi nh·∫•t
                    latest_chapter = ""
                    chapter_el = item.select_one(".comic-item .chapter a") or item.select_one(".chapter a")
                    if chapter_el:
                        latest_chapter = chapter_el.get_text(strip=True)
                    
                    manga_list.append({
                        "id": manga_id,
                        "title": title_el.get_text(strip=True),
                        "url": href,
                        "thumbnail": thumbnail,
                        "thumbnail_original": thumbnail_original,
                        "latest_chapter": latest_chapter
                    })
                    
                    print(f"  [{idx+1}/{len(items)}] {title_el.get_text(strip=True)[:30]}...")
            
            # L∆∞u v√†o MongoDB
            db.save_manga_list(manga_list)
            print(f"‚òÅÔ∏è ƒê√£ l∆∞u {len(manga_list)} truy·ªán v√†o MongoDB")
            
            context.close()
            return manga_list

    def crawl_story_detail(self, manga_id, download_cover=True):
        """Crawl chi ti·∫øt m·ªôt truy·ªán - L∆ØU V√ÄO MONGODB"""
        url = f"{self.base_url}/truyen-tranh/{manga_id}"
        print(f"üìñ ƒêang crawl chi ti·∫øt truy·ªán: {manga_id}")
        
        # ∆Øu ti√™n d√πng FlareSolverr tr√™n production
        if self.use_flaresolverr:
            data = self._crawl_story_via_flaresolverr(manga_id, url, download_cover)
            if data and data.get('title'):
                return data
            print("‚ö†Ô∏è FlareSolverr th·∫•t b·∫°i, th·ª≠ Playwright...")
        
        # Fallback: S·ª≠ d·ª•ng Playwright
        return self._crawl_story_via_playwright(manga_id, url, download_cover)
    
    def _crawl_story_via_flaresolverr(self, manga_id, url, download_cover=True):
        """Crawl story detail qua FlareSolverr"""
        print(f"üîì ƒêang bypass Cloudflare qua FlareSolverr...")
        
        result = flaresolverr.get_page(url)
        if not result or not result.get("html"):
            print("‚ùå FlareSolverr kh√¥ng th·ªÉ l·∫•y ƒë∆∞·ª£c trang")
            return None
        
        # L∆∞u cookies t·ª´ FlareSolverr ƒë·ªÉ d√πng cho requests
        self.cf_cookies = result.get("cookies", [])
        
        html = result["html"]
        soup = BeautifulSoup(html, "lxml")
        
        # L·∫•y v√† upload thumbnail qua requests
        thumbnail_original = ""
        thumb_el = soup.select_one(".col-image img")
        if thumb_el:
            thumbnail_original = thumb_el.get('data-original') or thumb_el.get('data-src') or thumb_el.get('src', '')
        
        thumbnail = thumbnail_original
        if download_cover and thumbnail_original:
            thumbnail = self.upload_cover_via_requests(manga_id, thumbnail_original)
        
        return self._parse_story_detail(soup, manga_id, download_cover, thumbnail, thumbnail_original)
    
    def _crawl_story_via_playwright(self, manga_id, url, download_cover=True):
        """Crawl story detail qua Playwright (fallback)"""
        from playwright.sync_api import sync_playwright
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            page.wait_for_timeout(2000)
            
            print("üìú ƒêang ph√¢n t√≠ch chapters...")
            
            content = page.content()
            soup = BeautifulSoup(content, "lxml")
            
            # Upload cover n·∫øu d√πng Playwright
            thumbnail_original = ""
            thumb_el = soup.select_one(".col-image img")
            if thumb_el:
                thumbnail_original = thumb_el.get('data-original') or thumb_el.get('data-src') or thumb_el.get('src', '')
            
            thumbnail = thumbnail_original
            if download_cover and thumbnail_original:
                thumbnail = self.upload_cover(page, manga_id, thumbnail_original)
            
            data = self._parse_story_detail(soup, manga_id, download_cover, thumbnail, thumbnail_original)
            
            context.close()
            return data
    
    def _parse_story_detail(self, soup, manga_id, download_cover=True, thumbnail=None, thumbnail_original=None):
        """Parse HTML ƒë·ªÉ l·∫•y th√¥ng tin truy·ªán"""
        print("üìú ƒêang ph√¢n t√≠ch chapters...")
        
        # L·∫•y th√¥ng tin truy·ªán
        title = ""
        title_el = soup.select_one("h1.title-detail")
        if title_el:
            title = title_el.get_text(strip=True)
        
        description = ""
        desc_el = soup.select_one(".detail-content p")
        if desc_el:
            description = desc_el.get_text(strip=True)
        
        # L·∫•y thumbnail n·∫øu ch∆∞a c√≥
        if not thumbnail_original:
            thumb_el = soup.select_one(".col-image img")
            if thumb_el:
                thumbnail_original = thumb_el.get('data-original') or thumb_el.get('data-src') or thumb_el.get('src', '')
            thumbnail = thumbnail_original
        
        # L·∫•y th·ªÉ lo·∫°i
        genres = []
        genre_els = soup.select(".kind.row .col-xs-8 a")
        for g in genre_els:
            genres.append(g.get_text(strip=True))
        
        # L·∫•y t√°c gi·∫£
        author = ""
        author_el = soup.select_one(".author.row .col-xs-8")
        if author_el:
            author = author_el.get_text(strip=True)
        
        # L·∫•y tr·∫°ng th√°i
        status = ""
        status_el = soup.select_one(".status.row .col-xs-8")
        if status_el:
            status = status_el.get_text(strip=True)
        
        # Ph√¢n t√≠ch pattern chapters
        visible_rows = soup.select("#nt_listchapter ul li.row:not(.heading)")
        
        chapters = []
        chapter_pattern = None
        max_chapter = 0
        min_chapter = float('inf')
        
        for row in visible_rows:
            link = row.select_one("a")
            if link:
                chap_url = link.get('href', '')
                
                url_match = re.search(r'[/-](chuong|chap|chapter)[/-]?(\d+)', chap_url, re.IGNORECASE)
                if url_match:
                    chapter_num = int(url_match.group(2))
                    prefix = url_match.group(1).lower()
                    
                    if not chapter_pattern:
                        base_url = re.sub(r'[/-](chuong|chap|chapter)[/-]?\d+.*$', '', chap_url, flags=re.IGNORECASE)
                        chapter_pattern = {
                            'base_url': base_url,
                            'prefix': prefix,
                            'separator': '-' if f'{prefix}-' in chap_url.lower() else ''
                        }
                    
                    max_chapter = max(max_chapter, chapter_num)
                    min_chapter = min(min_chapter, chapter_num)
        
        print(f"  üìä Ph√¢n t√≠ch: Chapter {min_chapter} ‚Üí {max_chapter}")
        
        # Generate t·∫•t c·∫£ chapters
        if chapter_pattern and max_chapter > 0:
            for i in range(max_chapter, -1, -1):
                chap_id = f"{chapter_pattern['prefix']}{chapter_pattern['separator']}{i}"
                chap_url = f"{chapter_pattern['base_url']}/{chap_id}"
                
                if not chap_url.startswith('http'):
                    chap_url = self.base_url + chap_url
                
                chapters.append({
                    "id": chap_id,
                    "name": f"Chapter {i}",
                    "url": chap_url
                })
            
            print(f"  ‚úÖ ƒê√£ generate {len(chapters)} chapters!")
        else:
            # Fallback
            for row in visible_rows:
                link = row.select_one("a")
                if link:
                    chap_url = link.get('href', '')
                    chap_id = chap_url.split('/')[-1] if chap_url else ''
                    if not chap_url.startswith('http'):
                        chap_url = self.base_url + chap_url
                    chapters.append({
                        "id": chap_id,
                        "name": link.get_text(strip=True),
                        "url": chap_url
                    })
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        data = {
            "id": manga_id,
            "title": title,
            "description": description,
            "thumbnail": thumbnail,
            "thumbnail_original": thumbnail_original,
            "author": author,
            "status": status,
            "genres": genres,
            "chapters": chapters,
            "total_chapters": len(chapters)
        }
        
        # L∆∞u v√†o MongoDB (c·∫£ manga_details v√† mangas)
        db.save_manga_detail(data)
        
        # Th√™m v√†o danh s√°ch manga tr√™n trang ch·ªß
        manga_item = {
            "id": manga_id,
            "title": title,
            "url": f"{self.base_url}/truyen-tranh/{manga_id}",
            "thumbnail": thumbnail,
            "thumbnail_original": thumbnail_original,
            "latest_chapter": chapters[0]["name"] if chapters else ""
        }
        db.save_manga_list([manga_item])
        
        print(f"‚òÅÔ∏è ƒê√£ l∆∞u '{title}' v·ªõi {len(chapters)} chapters v√†o MongoDB")
        
        return data

    def _download_chapter_via_flaresolverr(self, manga_id, chapter_id, chapter_url):
        """Download chapter s·ª≠ d·ª•ng FlareSolverr - Download + Upload song song"""
        print(f"üîì ƒêang bypass Cloudflare qua FlareSolverr...")
        
        result = flaresolverr.get_page(chapter_url)
        if not result or not result.get("html"):
            print("‚ùå FlareSolverr kh√¥ng th·ªÉ l·∫•y ƒë∆∞·ª£c trang")
            return []
        
        html = result["html"]
        soup = BeautifulSoup(html, "lxml")
        
        # T√¨m t·∫•t c·∫£ ·∫£nh chapter
        imgs = soup.select(".reading-detail img, .page-chapter img, .reading img")
        
        if not imgs:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ·∫£nh trong chapter")
            return []
        
        print(f"‚òÅÔ∏è T√¨m th·∫•y {len(imgs)} ·∫£nh. Download + Upload song song...")
        
        folder_path = f"manga/{manga_id}/{chapter_id}"
        
        # C·∫≠p nh·∫≠t cookies t·ª´ FlareSolverr v√†o session
        self._update_session_cookies(result.get("cookies", []))
        
        if result.get("user_agent"):
            self.session.headers["User-Agent"] = result["user_agent"]
        
        # Download v√† Upload song song trong c√πng 1 task
        def download_and_upload(item):
            idx, img = item
            src = img.get("data-original") or img.get("data-src") or img.get("src")
            if not src:
                return None
            if "http" not in src:
                if src.startswith("//"):
                    src = "https:" + src
                else:
                    return None
            try:
                # Download
                response = self.session.get(src, timeout=30)
                if response.status_code == 200 and len(response.content) > 1000:
                    # Upload ngay sau khi download xong
                    filename = f"{idx:03d}.jpg"
                    url = image_storage.upload_from_bytes(response.content, folder_path, filename)
                    if url:
                        return (idx, url)
            except Exception as e:
                print(f"  ‚ùå ·∫¢nh {idx} l·ªói: {e}")
            return None
        
        # Ch·∫°y song song: download + upload c√πng l√∫c
        urls = [None] * len(imgs)
        completed = 0
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(download_and_upload, (idx, img)): idx for idx, img in enumerate(imgs)}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    idx, url = result
                    urls[idx] = url
                    completed += 1
                    print(f"  ‚òÅÔ∏è [{completed}/{len(imgs)}] Downloaded + Uploaded")
        
        # L·ªçc b·ªè None
        urls = [url for url in urls if url]
        print(f"‚úÖ Ho√†n th√†nh {len(urls)}/{len(imgs)} ·∫£nh")
        
        return urls

    def download_chapter_images(self, manga_id, chapter_id, chapter_url=None):
        """T·∫£i v√† upload ·∫£nh chapter l√™n ImageKit - L∆ØU URLs V√ÄO MONGODB"""
        if not chapter_url:
            chapter_url = f"{self.base_url}/truyen-tranh/{manga_id}/{chapter_id}"
        
        # Ki·ªÉm tra ƒë√£ c√≥ tr√™n cloud ch∆∞a
        existing_urls = db.get_chapter_images(manga_id, chapter_id)
        if existing_urls:
            print(f"‚è≠Ô∏è Chapter {chapter_id} ƒë√£ c√≥ tr√™n cloud ({len(existing_urls)} ·∫£nh)")
            return existing_urls
        
        print(f"üì• ƒêang t·∫£i v√† upload chapter: {chapter_id}")
        
        # ∆Øu ti√™n s·ª≠ d·ª•ng FlareSolverr ƒë·ªÉ bypass Cloudflare (cho production)
        if self.use_flaresolverr:
            urls = self._download_chapter_via_flaresolverr(manga_id, chapter_id, chapter_url)
            if urls:
                db.save_chapter_images(manga_id, chapter_id, urls)
                print(f"‚òÅÔ∏è ƒê√£ l∆∞u {len(urls)} URLs v√†o MongoDB (via FlareSolverr)")
                return urls
            else:
                print("‚ö†Ô∏è FlareSolverr th·∫•t b·∫°i, th·ª≠ Playwright...")
        
        # Fallback: S·ª≠ d·ª•ng Playwright (ho·∫°t ƒë·ªông t·ªët tr√™n local)
        try:
            urls = self._download_chapter_via_playwright(manga_id, chapter_id, chapter_url)
            if urls:
                db.save_chapter_images(manga_id, chapter_id, urls)
                print(f"‚òÅÔ∏è ƒê√£ l∆∞u {len(urls)} URLs v√†o MongoDB")
            return urls
        except Exception as e:
            print(f"‚ùå L·ªói Playwright: {e}")
            return []

    def _download_chapter_via_playwright(self, manga_id, chapter_id, chapter_url):
        """Download chapter s·ª≠ d·ª•ng Playwright (fallback khi kh√¥ng c√≥ FlareSolverr)"""
        from playwright.sync_api import sync_playwright
        
        print("üé≠ ƒêang s·ª≠ d·ª•ng Playwright (fallback)...")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            
            # Additional anti-detection measures
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                window.chrome = {runtime: {}};
            """)
            
            page.goto(chapter_url, wait_until="domcontentloaded", timeout=60000)
            
            # BYPASS LOGIC 2.0
            max_retries = 3
            for attempt in range(max_retries):
                page_title = page.title()
                print(f"  üìÑ [{attempt+1}/{max_retries}] Page Title: {page_title}")
                
                if "Just a moment" in page_title or "Attention Required" in page_title or "Cloudflare" in page_title:
                    print("  üõ°Ô∏è Detect Cloudflare! Waiting for redirect...")
                    page.wait_for_timeout(5000)
                    
                    try:
                         frames = page.frames
                         for frame in frames:
                             if "challenge" in frame.url:
                                 print("  üñ±Ô∏è Found Challenge Frame, trying to interact...")
                                 frame.click("body", timeout=2000)
                    except: pass
                    
                    page.wait_for_timeout(5000)
                else:
                    break
            
            # Cu·ªôn trang ƒë·ªÉ load lazy images
            print("üìú ƒêang k√≠ch ho·∫°t lazy loading...")
            for i in range(10):
                page.mouse.wheel(0, 1000)
                page.wait_for_timeout(800)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(2000)
            
            try:
                page.wait_for_selector(".reading-detail img, .page-chapter img", timeout=5000)
            except:
                print("  ‚ö†Ô∏è Timeout ch·ªù ·∫£nh...")

            # L·∫•y t·∫•t c·∫£ ·∫£nh
            imgs = page.query_selector_all(".reading-detail img, .page-chapter img, .reading img, #image-0")
            
            folder_path = f"manga/{manga_id}/{chapter_id}"
            
            print(f"‚òÅÔ∏è T√¨m th·∫•y {len(imgs)} ·∫£nh. Download + Upload song song...")
            
            # Thu th·∫≠p t·∫•t c·∫£ src tr∆∞·ªõc
            img_sources = []
            for idx, img in enumerate(imgs):
                src = img.get_attribute("data-original") or img.get_attribute("data-src") or img.get_attribute("src")
                if src:
                    if "http" not in src:
                        if src.startswith("//"):
                            src = "https:" + src
                        else:
                            continue
                    img_sources.append((idx, src))
            
            # T·∫£i ·∫£nh qua Playwright v√† l∆∞u v√†o dict
            downloaded = {}
            for idx, src in img_sources:
                try:
                    response = page.request.get(src, headers={"referer": self.base_url + "/"})
                    if response.status == 200:
                        downloaded[idx] = response.body()
                        print(f"  üì• Downloaded {len(downloaded)}/{len(img_sources)}")
                except Exception as e:
                    print(f"  ‚ùå L·ªói download {idx}: {e}")
            
            context.close()
        
        # Upload song song sau khi ƒë√≥ng browser
        if downloaded:
            print(f"‚òÅÔ∏è Upload {len(downloaded)} ·∫£nh song song...")
            
            def upload_one(item):
                idx, data = item
                filename = f"{idx:03d}.jpg"
                return idx, image_storage.upload_from_bytes(data, folder_path, filename)
            
            urls = [None] * (max(downloaded.keys()) + 1)
            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = {executor.submit(upload_one, item): item[0] for item in downloaded.items()}
                for future in as_completed(futures):
                    try:
                        idx, url = future.result()
                        if url:
                            urls[idx] = url
                            print(f"  ‚òÅÔ∏è Uploaded {sum(1 for u in urls if u)}/{len(downloaded)}")
                    except Exception as e:
                        print(f"  ‚ùå Upload error: {e}")
            
            return [url for url in urls if url]
        
        return []

    def get_manga_list(self):
        """L·∫•y danh s√°ch manga t·ª´ MongoDB"""
        mangas = db.get_manga_list(limit=200)
        # Chuy·ªÉn ObjectId th√†nh string
        for m in mangas:
            if '_id' in m:
                m['_id'] = str(m['_id'])
        return mangas

    def get_story_data(self, manga_id):
        """L·∫•y chi ti·∫øt truy·ªán t·ª´ MongoDB"""
        data = db.get_manga_detail(manga_id)
        if data and '_id' in data:
            data['_id'] = str(data['_id'])
        return data

    def get_chapter_images(self, manga_id, chapter_id):
        """L·∫•y danh s√°ch URLs ·∫£nh t·ª´ MongoDB"""
        return db.get_chapter_images(manga_id, chapter_id)
    
    def get_download_status(self, manga_id):
        """L·∫•y tr·∫°ng th√°i t·∫£i t·ª´ MongoDB"""
        return db.get_download_status(manga_id)
        
    def get_downloaded_chapters(self, manga_id):
        """L·∫•y danh s√°ch chapter IDs ƒë√£ t·∫£i"""
        return db.get_downloaded_chapters(manga_id)


# CLI Interface
if __name__ == "__main__":
    import sys
    
    crawler = MangaCrawler()
    
    if len(sys.argv) < 2:
        print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           üéå MANGA CRAWLER - CLOUD ONLY MODE üéå           ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  L∆∞u tr·ªØ: MongoDB (data) + ImageKit (·∫£nh 20GB free)       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  C√°ch s·ª≠ d·ª•ng:                                            ‚ïë
‚ïë   python manga_crawler.py home                            ‚ïë
‚ïë   python manga_crawler.py story <manga-id>                ‚ïë
‚ïë   python manga_crawler.py chapter <manga-id> <chapter-id> ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """)
        sys.exit(0)
    
    command = sys.argv[1]
    
    if command == "home":
        crawler.crawl_home()
    
    elif command == "story" and len(sys.argv) >= 3:
        manga_id = sys.argv[2]
        crawler.crawl_story_detail(manga_id)
    
    elif command == "chapter" and len(sys.argv) >= 4:
        manga_id = sys.argv[2]
        chapter_id = sys.argv[3]
        crawler.download_chapter_images(manga_id, chapter_id)
    
    else:
        print("‚ùå L·ªánh kh√¥ng h·ª£p l·ªá!")
        print("S·ª≠ d·ª•ng: python manga_crawler.py [home|story|chapter] [args...]")
