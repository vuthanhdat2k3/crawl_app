"""
Manga Crawler - Thu tháº­p dá»¯ liá»‡u tá»« NetTruyen
LÆ°u trá»¯: CHá»ˆ Sá»¬ Dá»¤NG CLOUD (MongoDB + ImageKit.io)
KhÃ´ng sá»­ dá»¥ng local storage
"""

import os
import re
import sys
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# Import database vÃ  image storage
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from database import db
from imagekit_storage import image_storage

class MangaCrawler:
    def __init__(self):
        self.base_url = "https://nettruyen.me.uk/"
        self.user_data_dir = os.path.join(os.path.dirname(__file__), "browser_profile")
        
        # Káº¿t ná»‘i cloud storage
        db.connect()
        image_storage.connect()
        print("â˜ï¸ Cloud-Only Mode: MongoDB + ImageKit")

    def _get_browser_context(self, playwright):
        """Táº¡o browser context vá»›i anti-bot"""
        return playwright.chromium.launch_persistent_context(
            self.user_data_dir,
            headless=True,
            args=["--disable-blink-features=AutomationControlled"],
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )

    def upload_cover(self, page, manga_id, thumbnail_url):
        """Táº£i vÃ  upload áº£nh bÃ¬a lÃªn ImageKit"""
        if not thumbnail_url:
            return None
        
        try:
            # Táº£i áº£nh tá»« nguá»“n
            response = page.request.get(thumbnail_url, headers={"referer": self.base_url + "/"})
            if response.status == 200:
                # Upload trá»±c tiáº¿p lÃªn ImageKit
                image_bytes = response.body()
                url = image_storage.upload_from_bytes(
                    image_bytes, 
                    "manga/covers", 
                    f"{manga_id}.jpg"
                )
                if url:
                    print(f"  â˜ï¸ Uploaded cover: {manga_id}")
                    return url
        except Exception as e:
            print(f"  âš ï¸ Lá»—i upload cover {manga_id}: {e}")
        
        return thumbnail_url  # Fallback vá» URL gá»‘c

    def crawl_home(self, download_covers=True):
        """Crawl danh sÃ¡ch manga tá»« trang chá»§ - LÆ¯U VÃ€O MONGODB"""
        print("ğŸŒ Äang crawl trang chá»§ NetTruyen...")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(self.base_url, wait_until="networkidle", timeout=60000)
            
            # Cuá»™n Ä‘á»ƒ load thÃªm
            for _ in range(3):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(1000)
            
            content = page.content()
            soup = BeautifulSoup(content, "lxml")
            
            manga_list = []
            items = soup.select(".item")
            
            for idx, item in enumerate(items):
                title_el = item.select_one("h3 a")
                img_el = item.select_one("img")
                
                if title_el:
                    href = title_el.get('href', '')
                    manga_id = href.split('/')[-1] if href else ''
                    
                    # Láº¥y thumbnail gá»‘c
                    thumbnail_original = ""
                    if img_el:
                        thumbnail_original = img_el.get('data-original') or img_el.get('data-src') or img_el.get('src', '')
                    
                    # Upload cover lÃªn ImageKit
                    thumbnail = thumbnail_original
                    if download_covers and thumbnail_original:
                        uploaded_url = self.upload_cover(page, manga_id, thumbnail_original)
                        if uploaded_url:
                            thumbnail = uploaded_url
                    
                    # Láº¥y chapter má»›i nháº¥t
                    latest_chapter = ""
                    chapter_el = item.select_one(".comic-item .chapter a") or item.select_one(".chapter a")
                    if chapter_el:
                        latest_chapter = chapter_el.get_text(strip=True)
                    
                    manga_list.append({
                        "id": manga_id,
                        "title": title_el.get_text(strip=True),
                        "url": href,
                        "thumbnail": thumbnail,
                        "thumbnail_original": thumbnail_original,
                        "latest_chapter": latest_chapter
                    })
                    
                    print(f"  [{idx+1}/{len(items)}] {title_el.get_text(strip=True)[:30]}...")
            
            # LÆ°u vÃ o MongoDB
            db.save_manga_list(manga_list)
            print(f"â˜ï¸ ÄÃ£ lÆ°u {len(manga_list)} truyá»‡n vÃ o MongoDB")
            
            context.close()
            return manga_list

    def crawl_story_detail(self, manga_id, download_cover=True):
        """Crawl chi tiáº¿t má»™t truyá»‡n - LÆ¯U VÃ€O MONGODB"""
        url = f"{self.base_url}/truyen-tranh/{manga_id}"
        print(f"ğŸ“– Äang crawl chi tiáº¿t truyá»‡n: {manga_id}")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            page.wait_for_timeout(2000)
            
            print("ğŸ“œ Äang phÃ¢n tÃ­ch chapters...")
            
            content = page.content()
            soup = BeautifulSoup(content, "lxml")
            
            # Láº¥y thÃ´ng tin truyá»‡n
            title = ""
            title_el = soup.select_one("h1.title-detail")
            if title_el:
                title = title_el.get_text(strip=True)
            
            description = ""
            desc_el = soup.select_one(".detail-content p")
            if desc_el:
                description = desc_el.get_text(strip=True)
            
            # Láº¥y vÃ  upload thumbnail
            thumbnail_original = ""
            thumbnail = ""
            thumb_el = soup.select_one(".col-image img")
            if thumb_el:
                thumbnail_original = thumb_el.get('data-original') or thumb_el.get('data-src') or thumb_el.get('src', '')
            
            if download_cover and thumbnail_original:
                thumbnail = self.upload_cover(page, manga_id, thumbnail_original)
            else:
                thumbnail = thumbnail_original
            
            # Láº¥y thá»ƒ loáº¡i
            genres = []
            genre_els = soup.select(".kind.row .col-xs-8 a")
            for g in genre_els:
                genres.append(g.get_text(strip=True))
            
            # Láº¥y tÃ¡c giáº£
            author = ""
            author_el = soup.select_one(".author.row .col-xs-8")
            if author_el:
                author = author_el.get_text(strip=True)
            
            # PhÃ¢n tÃ­ch pattern chapters
            visible_rows = soup.select("#nt_listchapter ul li.row:not(.heading)")
            
            chapters = []
            chapter_pattern = None
            max_chapter = 0
            min_chapter = float('inf')
            
            for row in visible_rows:
                link = row.select_one("a")
                if link:
                    chap_url = link.get('href', '')
                    
                    url_match = re.search(r'[/-](chuong|chap|chapter)[/-]?(\d+)', chap_url, re.IGNORECASE)
                    if url_match:
                        chapter_num = int(url_match.group(2))
                        prefix = url_match.group(1).lower()
                        
                        if not chapter_pattern:
                            base_url = re.sub(r'[/-](chuong|chap|chapter)[/-]?\d+.*$', '', chap_url, flags=re.IGNORECASE)
                            chapter_pattern = {
                                'base_url': base_url,
                                'prefix': prefix,
                                'separator': '-' if f'{prefix}-' in chap_url.lower() else ''
                            }
                        
                        max_chapter = max(max_chapter, chapter_num)
                        min_chapter = min(min_chapter, chapter_num)
            
            print(f"  ğŸ“Š PhÃ¢n tÃ­ch: Chapter {min_chapter} â†’ {max_chapter}")
            
            # Generate táº¥t cáº£ chapters
            if chapter_pattern and max_chapter > 0:
                for i in range(max_chapter, -1, -1):
                    chap_id = f"{chapter_pattern['prefix']}{chapter_pattern['separator']}{i}"
                    chap_url = f"{chapter_pattern['base_url']}/{chap_id}"
                    
                    if not chap_url.startswith('http'):
                        chap_url = self.base_url + chap_url
                    
                    chapters.append({
                        "id": chap_id,
                        "name": f"Chapter {i}",
                        "url": chap_url
                    })
                
                print(f"  âœ… ÄÃ£ generate {len(chapters)} chapters!")
            else:
                # Fallback
                for row in visible_rows:
                    link = row.select_one("a")
                    if link:
                        chap_url = link.get('href', '')
                        chap_id = chap_url.split('/')[-1] if chap_url else ''
                        if not chap_url.startswith('http'):
                            chap_url = self.base_url + chap_url
                        chapters.append({
                            "id": chap_id,
                            "name": link.get_text(strip=True),
                            "url": chap_url
                        })
            
            # Chuáº©n bá»‹ dá»¯ liá»‡u
            data = {
                "id": manga_id,
                "title": title,
                "description": description,
                "thumbnail": thumbnail,
                "thumbnail_original": thumbnail_original,
                "author": author,
                "genres": genres,
                "chapters": chapters,
                "total_chapters": len(chapters)
            }
            
            # LÆ°u vÃ o MongoDB
            db.save_manga_detail(data)
            print(f"â˜ï¸ ÄÃ£ lÆ°u '{title}' vá»›i {len(chapters)} chapters vÃ o MongoDB")
            
            context.close()
            return data

    def download_chapter_images(self, manga_id, chapter_id, chapter_url=None):
        """Táº£i vÃ  upload áº£nh chapter lÃªn ImageKit - LÆ¯U URLs VÃ€O MONGODB"""
        if not chapter_url:
            chapter_url = f"{self.base_url}/truyen-tranh/{manga_id}/{chapter_id}"
        
        # Kiá»ƒm tra Ä‘Ã£ cÃ³ trÃªn cloud chÆ°a
        existing_urls = db.get_chapter_images(manga_id, chapter_id)
        if existing_urls:
            print(f"â­ï¸ Chapter {chapter_id} Ä‘Ã£ cÃ³ trÃªn cloud ({len(existing_urls)} áº£nh)")
            return existing_urls
        
        print(f"ğŸ“¥ Äang táº£i vÃ  upload chapter: {chapter_id}")
        
        with sync_playwright() as p:
            context = self._get_browser_context(p)
            page = context.new_page()
            page.goto(chapter_url, wait_until="domcontentloaded", timeout=60000)
            
            # Cuá»™n trang Ä‘á»ƒ kÃ­ch hoáº¡t lazy loading
            print("ğŸ“œ Äang load áº£nh...")
            for i in range(5):
                page.evaluate(f"window.scrollTo(0, {(i+1) * 2000})")
                page.wait_for_timeout(500)
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(2000)
            
            # Láº¥y táº¥t cáº£ áº£nh
            imgs = page.query_selector_all(".reading-detail img, .page-chapter img")
            
            urls = []
            folder_path = f"manga/{manga_id}/{chapter_id}"
            
            print(f"â˜ï¸ Äang upload {len(imgs)} áº£nh lÃªn ImageKit...")
            
            for idx, img in enumerate(imgs):
                src = img.get_attribute("data-src") or img.get_attribute("data-original") or img.get_attribute("src")
                if not src or "http" not in src:
                    continue
                
                try:
                    response = page.request.get(src, headers={"referer": self.base_url + "/"})
                    if response.status == 200:
                        filename = f"{idx:03d}.jpg"
                        
                        # Upload trá»±c tiáº¿p lÃªn ImageKit
                        url = image_storage.upload_from_bytes(
                            response.body(),
                            folder_path,
                            filename
                        )
                        
                        if url:
                            urls.append(url)
                            print(f"  â˜ï¸ [{idx+1}/{len(imgs)}] Uploaded")
                        else:
                            print(f"  âŒ [{idx+1}/{len(imgs)}] Upload failed")
                except Exception as e:
                    print(f"  âŒ Lá»—i áº£nh {idx}: {e}")
            
            context.close()
            
            # LÆ°u URLs vÃ o MongoDB
            if urls:
                db.save_chapter_images(manga_id, chapter_id, urls)
                print(f"â˜ï¸ ÄÃ£ lÆ°u {len(urls)} URLs vÃ o MongoDB")
            
            return urls

    def get_manga_list(self):
        """Láº¥y danh sÃ¡ch manga tá»« MongoDB"""
        mangas = db.get_manga_list(limit=200)
        # Chuyá»ƒn ObjectId thÃ nh string
        for m in mangas:
            if '_id' in m:
                m['_id'] = str(m['_id'])
        return mangas

    def get_story_data(self, manga_id):
        """Láº¥y chi tiáº¿t truyá»‡n tá»« MongoDB"""
        data = db.get_manga_detail(manga_id)
        if data and '_id' in data:
            data['_id'] = str(data['_id'])
        return data

    def get_chapter_images(self, manga_id, chapter_id):
        """Láº¥y danh sÃ¡ch URLs áº£nh tá»« MongoDB"""
        return db.get_chapter_images(manga_id, chapter_id)
    
    def get_download_status(self, manga_id):
        """Láº¥y tráº¡ng thÃ¡i táº£i tá»« MongoDB"""
        return db.get_download_status(manga_id)
        
    def get_downloaded_chapters(self, manga_id):
        """Láº¥y danh sÃ¡ch chapter IDs Ä‘Ã£ táº£i"""
        return db.get_downloaded_chapters(manga_id)


# CLI Interface
if __name__ == "__main__":
    import sys
    
    crawler = MangaCrawler()
    
    if len(sys.argv) < 2:
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸŒ MANGA CRAWLER - CLOUD ONLY MODE ğŸŒ           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  LÆ°u trá»¯: MongoDB (data) + ImageKit (áº£nh 20GB free)       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CÃ¡ch sá»­ dá»¥ng:                                            â•‘
â•‘   python manga_crawler.py home                            â•‘
â•‘   python manga_crawler.py story <manga-id>                â•‘
â•‘   python manga_crawler.py chapter <manga-id> <chapter-id> â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        sys.exit(0)
    
    command = sys.argv[1]
    
    if command == "home":
        crawler.crawl_home()
    
    elif command == "story" and len(sys.argv) >= 3:
        manga_id = sys.argv[2]
        crawler.crawl_story_detail(manga_id)
    
    elif command == "chapter" and len(sys.argv) >= 4:
        manga_id = sys.argv[2]
        chapter_id = sys.argv[3]
        crawler.download_chapter_images(manga_id, chapter_id)
    
    else:
        print("âŒ Lá»‡nh khÃ´ng há»£p lá»‡!")
        print("Sá»­ dá»¥ng: python manga_crawler.py [home|story|chapter] [args...]")
